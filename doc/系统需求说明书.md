# Agent自动生成与优化系统 - 需求说明书

## 1. 项目目标

开发一个**基于LangChain的Web应用程序**，能够根据用户输入的**角色名称、需求以及期望的输出格式**，自动生成并优化一个**Agent的prompt**。该prompt需要赋予Agent完整的工作能力（调用工具、执行任务），并确保Agent的输出符合用户约定的格式（如Markdown、JSON、YAML等）。系统通过**自动迭代测试-评估-优化**来提升prompt质量，在达到设定阈值后暂停，交由**人工介入**修改或继续优化。整个过程在Web界面上可视化，并保存所有历史数据到本地数据库（SQLite）。

---

## 2. 核心功能模块

### 2.1 用户输入
在Web界面上输入：
- Agent角色（如"文案策划"）
- 具体需求描述
- **输出格式约定**（如Markdown、JSON、YAML、pdf、word、csv等，可多选或自定义）
- 可配置的参数（最大迭代次数、评分阈值、选择基础模型和评估模型等）

### 2.2 初始prompt生成
根据用户输入（包括输出格式要求），由选定的本地模型（Ollama部署）生成初始prompt。prompt中应明确指示Agent的输出必须遵循约定的格式。

### 2.3 测试任务自动生成
由本地模型（或可选模型）根据角色生成若干测试任务（如3个），确保与角色强相关，并且任务设计能检验Agent是否遵循输出格式。

### 2.4 Agent执行测试
使用当前prompt实例化Agent，并让它依次执行测试任务。Agent应能调用多种工具（搜索引擎、计算器、Python REPL、文件读写等）完成任务，并**严格按照prompt中约定的格式输出**。

### 2.5 评估
另一个模型（可从Ollama列表选择或外部API，支持用户添加API）对每个任务的输出进行评分，提供文字反馈。系统计算平均分。

### 2.6 优化
将当前prompt、测试结果（包括格式符合情况）和评估反馈输入优化模型（可选），生成改进后的新prompt。优化过程会特别关注如何更好地引导Agent遵循格式约定。

### 2.7 迭代控制
自动迭代直到满足停止条件。满足条件后暂停，通知用户人工介入。

### 2.8 人工介入
用户在界面上可以查看每次迭代的详情（prompt、任务、输出、评分、反馈），并可修改当前prompt（包括调整格式约定的描述）、调整测试任务，然后手动触发继续优化（从修改后的prompt开始，保留历史）。

### 2.9 数据持久化
使用SQLite数据库保存所有创建的Agent、每次迭代的详细记录（prompt版本、任务列表、每个任务的输出、评分、反馈、**格式要求**、时间戳等），支持按角色查询历史。

### 2.10 可视化
Web界面需提供：
- 输入表单（角色、需求、**输出格式选项**、参数配置）
- 实时显示当前迭代进度和输出（流式或逐步刷新）
- 每次迭代的详细面板（prompt、任务结果、评分、**格式检查结果**等）
- 评分变化趋势图表
- 人工介入时的编辑界面（修改prompt、任务）
- 历史记录查看

---

## 3. 停止条件（细化）

系统在以下任一条件满足时停止迭代：

| 条件 | 说明 | 可配置 |
|------|------|--------|
| 平均分达到阈值 | 平均分 ≥ 用户设定的评分阈值 | ✅ 阈值可配置 |
| 达到最大迭代次数 | 迭代次数 ≥ 最大迭代次数限制 | ✅ 次数可配置 |
| 早停机制 | 连续N次迭代平均分提升幅度 < 设定值 | ✅ N和阈值可配置 |
| 完美达标 | 单次迭代所有任务评分均为5分 | ❌ 固定规则 |
| 用户手动终止 | 用户点击"停止"按钮 | ❌ 固定功能 |

**早停机制参数**：
- `early_stop_patience`：连续多少次无提升时停止（默认：3）
- `early_stop_threshold`：提升幅度阈值（默认：0.1分）

---

## 4. 评估标准（量化）

### 4.1 评分维度

评估采用多维度加权评分，总分1-5分：

| 维度 | 权重 | 评分标准 |
|------|------|----------|
| 内容质量 | 40% | 相关性、准确性、完整性 |
| 格式符合度 | 25% | 是否严格遵循约定的输出格式 |
| 工具使用合理性 | 20% | 工具选择是否恰当、调用是否有效 |
| 创意性 | 15% | 输出的创新性和独特性 |

### 4.2 评分细则（Rubric）

**内容质量（1-5分）**：
- 5分：完全符合任务要求，内容丰富准确，逻辑清晰
- 4分：基本符合要求，内容较完整，有小瑕疵
- 3分：部分符合要求，有遗漏或不准确之处
- 2分：与要求有较大偏差，内容不完整
- 1分：完全不符合要求或无有效输出

**格式符合度（1-5分）**：
- 5分：完全符合约定格式，可直接解析使用
- 4分：基本符合格式，有轻微格式问题
- 3分：部分符合格式，需要手动调整
- 2分：格式问题较多，难以直接使用
- 1分：完全不符合格式要求

**工具使用合理性（1-5分）**：
- 5分：工具选择精准，调用高效，结果有效
- 4分：工具选择合理，调用基本正确
- 3分：使用了工具但效果一般
- 2分：工具选择不当或调用失败
- 1分：未使用工具或使用完全错误

**创意性（1-5分）**：
- 5分：输出富有创意，超出预期
- 4分：有一定创意，有亮点
- 3分：中规中矩，符合预期
- 2分：缺乏创意，较为机械
- 1分：完全照搬或无意义输出

### 4.3 最终得分计算

```
最终得分 = 内容质量 × 0.4 + 格式符合度 × 0.25 + 工具使用合理性 × 0.2 + 创意性 × 0.15
```

---

## 5. 异常场景处理

### 5.1 模型调用异常

| 异常类型 | 处理策略 |
|----------|----------|
| 调用超时 | 自动重试（最多3次），每次重试间隔递增（2s, 4s, 8s） |
| 模型返回空 | 记录错误，评分记为1分，继续下一任务 |
| 模型返回格式错误 | 尝试JSON解析，失败则使用正则提取关键信息 |
| API密钥无效 | 立即停止，提示用户检查配置 |

### 5.2 Agent执行异常

| 异常类型 | 处理策略 |
|----------|----------|
| 任务执行超时 | 设置超时限制（默认60秒），超时后强制终止，记录为失败 |
| Agent陷入死循环 | 检测连续相同Action，超过阈值后强制终止 |
| 工具调用失败 | 记录错误信息，Agent可尝试其他方案或报告失败 |
| 内存溢出 | 设置内存限制，超出后重启Agent实例 |

### 5.3 评估模型异常

| 异常类型 | 处理策略 |
|----------|----------|
| 评分格式错误 | 尝试从文本中提取数字，失败则使用默认评分3分 |
| 评分超出范围 | 强制限制在1-5范围内（如6→5，0→1） |
| 反馈为空 | 使用默认反馈文本"评估完成，无详细反馈" |
| 评估模型不可用 | 使用备用评估模型（如果配置）或暂停等待 |

### 5.4 系统级异常

| 异常类型 | 处理策略 |
|----------|----------|
| 数据库写入失败 | 写入本地日志文件作为备份，定期重试 |
| 磁盘空间不足 | 检测并警告用户，暂停新任务 |
| 进程崩溃 | 保存当前状态到检查点，支持恢复 |

---

## 6. 技术栈要求

### 6.1 核心框架
- **LangChain**：>= 0.1.0（用于构建Agent、工具调用、链式处理）
- **Python**：>= 3.10

### 6.2 模型接入
- **Ollama**：本地部署，支持从Ollama列表中选择模型
- **外部API**：预留OpenAI兼容格式接口，允许用户添加自定义API

### 6.3 Web框架
- **推荐**：Streamlit（快速开发）
- **备选**：Flask/Django + 前端（Vue/React）

### 6.4 数据库
- **SQLite**：>= 3.35

### 6.5 工具集成
- **搜索引擎**：支持API调用（Google Custom Search、Bing Search），用户需提供密钥
- **计算器**：Python内置计算或简单表达式求值
- **Python REPL**：安全执行Python代码（沙箱环境）
- **文件读写**：允许Agent读写指定目录下的文件
- **扩展接口**：预留扩展接口，方便未来添加新工具

---

## 7. 关键流程细节

### 7.1 初始prompt生成
使用用户选择的模型，输入提示词：
```
根据以下需求，为{角色}角色创建一个详细的系统提示。
需求：{用户输入}
重要：Agent的输出必须严格遵循以下格式：{输出格式约定}
请在prompt中明确说明输出格式要求，并包含角色描述、核心能力、工具使用说明、工作流程等内容。
```

### 7.2 测试任务生成
使用选定模型，根据角色生成一组任务：
```
请为{角色}生成3个典型的测试任务，要求任务能充分检验该角色的能力，
每个任务需明确描述，适合作为LLM的输入。
同时，任务设计应能检验Agent是否遵循{输出格式约定}的输出格式。
```

### 7.3 Agent执行任务
LangChain Agent配置好工具，根据当前prompt执行任务，输出结果。系统增加**格式预检查**步骤（用正则或解析器快速验证是否符合JSON/YAML等），但最终以评估模型的判断为准。

### 7.4 评估
评估模型接收任务描述、Agent输出和约定的输出格式，输出结构化评分（JSON格式）。评估提示词：
```
请根据以下维度评分（各1-5分）：
1. 内容质量（相关性、准确性、完整性）
2. 格式符合度（是否严格遵循{输出格式约定}）
3. 工具使用合理性（工具选择和调用效果）
4. 创意性（输出的创新性）

输出JSON格式：
{
  "content_quality": 分数,
  "format_compliance": 分数,
  "tool_usage": 分数,
  "creativity": 分数,
  "final_score": 加权总分,
  "feedback": "详细改进意见"
}
```

### 7.5 优化
优化模型接收旧prompt、任务结果、评估反馈，输出新prompt：
```
根据反馈，修改prompt以提高Agent的内容质量和格式遵循度。
只输出新prompt，不要解释。
```

---

## 8. 用户交互与界面

### 8.1 主界面
- 输入框：Agent角色、需求描述
- **输出格式选择**：提供常见格式复选框（Markdown, JSON, YAML, XML, CSV, 纯文本等），支持"自定义格式"输入框
- 参数设置：最大迭代次数、评分阈值、早停参数、基础模型、评估模型、搜索引擎API密钥等
- 开始按钮

### 8.2 运行界面
- 显示当前迭代次数、平均分
- 每个任务的实时输出（滚动显示），标注**格式检查状态**
- 每次迭代完成后，展示本次的prompt、所有任务结果、评分、反馈
- 评分趋势图实时更新
- **停止按钮**：允许用户随时手动终止

### 8.3 暂停/人工介入
- 当满足停止条件或用户手动点击暂停时，界面显示"人工介入"区域
- 显示当前prompt的可编辑文本框，测试任务列表（可增删改），输出格式要求的可编辑字段
- 提供"保存修改并继续"按钮，从当前状态继续优化
- **Prompt版本对比**：可对比两次迭代的prompt差异

### 8.4 历史记录
- 侧边栏或单独页面列出所有创建的Agent（角色+时间戳）
- 点击可查看其完整迭代历史，包括每次迭代的格式要求
- 支持导出历史数据

---

## 9. 数据模型

### 9.1 Agent表
| 字段 | 类型 | 说明 |
|------|------|------|
| id | INTEGER | 主键 |
| role | TEXT | 角色名称 |
| user_requirement | TEXT | 用户需求描述 |
| output_format | TEXT | 输出格式约定 |
| created_at | TIMESTAMP | 创建时间 |
| final_prompt | TEXT | 最终采用的prompt |
| status | TEXT | 状态：created/running/paused/completed/failed |

### 9.2 Iteration表
| 字段 | 类型 | 说明 |
|------|------|------|
| id | INTEGER | 主键 |
| agent_id | INTEGER | 关联Agent |
| iteration_number | INTEGER | 迭代序号 |
| prompt | TEXT | 本次迭代使用的prompt |
| avg_score | REAL | 平均分 |
| scores_detail | TEXT | 各维度评分详情（JSON） |
| created_at | TIMESTAMP | 创建时间 |

### 9.3 TaskResult表
| 字段 | 类型 | 说明 |
|------|------|------|
| id | INTEGER | 主键 |
| iteration_id | INTEGER | 关联Iteration |
| task_description | TEXT | 任务描述 |
| output | TEXT | Agent的输出 |
| scores | TEXT | 各维度评分（JSON） |
| final_score | INTEGER | 最终评分（1-5） |
| feedback | TEXT | 评估反馈 |
| format_check | TEXT | 格式检查结果 |
| error_log | TEXT | 错误日志（如有） |
| created_at | TIMESTAMP | 创建时间 |

### 9.4 Task表
| 字段 | 类型 | 说明 |
|------|------|------|
| id | INTEGER | 主键 |
| agent_id | INTEGER | 关联Agent |
| task_description | TEXT | 任务描述 |
| difficulty | TEXT | 任务难度：easy/medium/hard |
| is_active | BOOLEAN | 是否在当前测试集中 |
| created_at | TIMESTAMP | 创建时间 |

### 9.5 ModelConfig表
| 字段 | 类型 | 说明 |
|------|------|------|
| id | INTEGER | 主键 |
| agent_id | INTEGER | 关联Agent |
| iteration_id | INTEGER | 关联Iteration（可为空） |
| model_type | TEXT | 模型类型：base/evaluator/optimizer/task_generator |
| model_source | TEXT | 来源：ollama/api |
| model_name | TEXT | 模型名称 |
| api_endpoint | TEXT | API端点（如使用外部API） |
| api_key_encrypted | TEXT | 加密存储的API密钥 |

---

## 10. 其他要求

### 10.1 实时输出
- 执行每个任务时能看到逐步输出（流式输出优先）
- 至少在每个任务完成时刷新显示

### 10.2 扩展性
- 模型接入和工具集成需设计为可插拔
- 输出格式的解析和验证预留扩展接口

### 10.3 安全性
- Python REPL采用沙箱环境（Docker容器或RestrictedPython）
- 文件读写限制在指定目录
- API密钥加密存储

### 10.4 错误处理
- 单个任务失败不影响整体流程
- 记录详细错误日志
- 提供错误恢复机制

### 10.5 性能要求
- 单次任务执行超时：60秒（可配置）
- 迭代间隔：支持用户配置
- 支持断点续传

---

## 11. 版本规划

### V1.0（基础版本）
- 核心迭代流程
- 基础评估和优化
- SQLite数据存储
- Streamlit界面

### V1.1（增强版本）
- 多维度量化评估
- 早停机制
- 异常处理完善
- 流式输出

### V1.2（高级版本）
- Prompt版本对比
- 任务难度分级
- 评估一致性检查
- 导出功能

import sys
import os
_project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
if _project_root not in sys.path:
    sys.path.insert(0, _project_root)

import streamlit as st
import sqlite3
import pandas as pd
import plotly.graph_objects as go
import json
import threading
import time
from typing import Optional, List, Dict, Any

from src.db.database import init_db, Database
from src.config.settings import settings
from src.core.model_client import ModelClient, get_ollama_models
from src.core.tools import get_tools
from src.core.agent_factory import create_agent
from src.core.evaluator import Evaluator
from src.utils.format_checker import FormatChecker


st.set_page_config(
    page_title="Agent Promptä¼˜åŒ–ç³»ç»Ÿ",
    page_icon="ðŸ¤–",
    layout="wide"
)


def get_or_create_optimization_state():
    if 'optimization_states' not in st.session_state:
        st.session_state['optimization_states'] = {}
    return st.session_state['optimization_states']


class OptimizationRunner:
    def __init__(self, agent_id: int, db: Database):
        self.agent_id = agent_id
        self.db = db
        self.agent = db.get_agent(agent_id)
        self.status = "åˆå§‹åŒ–"
        self.current_iteration = 0
        self.current_task = ""
        self.logs: List[str] = []
        self.prompt = ""
        self.avg_score = 0.0
        self.stop_requested = False
        self.completed = False
        self.error = None
        self.iteration_results: List[Dict[str, Any]] = []
        self.current_results: List[Dict[str, Any]] = []
        
        self._init_clients()
    
    def _init_clients(self):
        configs = self.db.get_model_configs(self.agent_id)
        
        def get_client(model_type: str, default_model: str) -> ModelClient:
            config = configs.get(model_type, {})
            return ModelClient(
                source=config.get('model_source', 'ollama'),
                model_name=config.get('model_name', default_model),
                api_base=config.get('api_endpoint'),
                api_key=config.get('api_key_encrypted')
            )
        
        self.base_client = get_client('base', settings.DEFAULT_MODEL)
        self.eval_client = get_client('evaluator', settings.DEFAULT_MODEL)
        self.optimizer_client = get_client('optimizer', settings.DEFAULT_MODEL)
        self.task_gen_client = get_client('task_generator', settings.DEFAULT_MODEL)
        
        self.tools = get_tools()
        self.evaluator = Evaluator(self.eval_client, self.agent['output_format'])
        self.format_checker = FormatChecker()
    
    def log(self, message: str):
        timestamp = time.strftime("%H:%M:%S")
        self.logs.append(f"[{timestamp}] {message}")
        if len(self.logs) > 100:
            self.logs = self.logs[-100:]
    
    def generate_initial_prompt(self) -> str:
        self.status = "ç”Ÿæˆåˆå§‹Prompt"
        self.log("æ­£åœ¨ç”Ÿæˆåˆå§‹Prompt...")
        
        system = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Promptå·¥ç¨‹å¸ˆã€‚æ ¹æ®ç”¨æˆ·éœ€æ±‚åˆ›å»ºè¯¦ç»†çš„Agentç³»ç»Ÿæç¤ºã€‚"
        user = f"""ä¸º{self.agent['role']}è§’è‰²åˆ›å»ºç³»ç»Ÿæç¤ºã€‚
éœ€æ±‚ï¼š{self.agent['user_requirement']}
è¾“å‡ºæ ¼å¼å¿…é¡»éµå¾ªï¼š{self.agent['output_format']}

è¯·åŒ…å«ï¼š
1. è§’è‰²æè¿°
2. æ ¸å¿ƒèƒ½åŠ›
3. å¯ç”¨å·¥å…·åŠä½¿ç”¨è¯´æ˜Ž
4. å·¥ä½œæµç¨‹
5. è¾“å‡ºæ ¼å¼è¦æ±‚ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰

åªè¾“å‡ºpromptå†…å®¹ï¼Œä¸è¦è§£é‡Šã€‚"""
        
        response = self.base_client.chat([
            {"role": "system", "content": system},
            {"role": "user", "content": user}
        ])
        self.log("åˆå§‹Promptç”Ÿæˆå®Œæˆ")
        return response
    
    def generate_tasks(self, num_tasks: int = 3) -> List[str]:
        tasks = self.db.get_active_tasks(self.agent_id)
        if tasks:
            return tasks
        
        self.status = "ç”Ÿæˆæµ‹è¯•ä»»åŠ¡"
        self.log("æ­£åœ¨ç”Ÿæˆæµ‹è¯•ä»»åŠ¡...")
        
        system = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä»»åŠ¡è®¾è®¡ä¸“å®¶ã€‚ä½ éœ€è¦è®¾è®¡èƒ½å¤Ÿå…¨é¢æµ‹è¯•Agentèƒ½åŠ›çš„å…·ä½“ä»»åŠ¡ã€‚

ä»»åŠ¡è®¾è®¡åŽŸåˆ™ï¼š
1. å…·ä½“æ€§ï¼šä»»åŠ¡å¿…é¡»åŒ…å«æ˜Žç¡®çš„è¾“å…¥æ•°æ®ã€çº¦æŸæ¡ä»¶å’Œé¢„æœŸè¾“å‡º
2. å¯æµ‹è¯•æ€§ï¼šä»»åŠ¡çš„è¾“å‡ºå¯ä»¥è¢«å®¢è§‚è¯„ä¼°
3. æ¸è¿›éš¾åº¦ï¼šä»Žç®€å•åˆ°å¤æ‚ï¼Œé€æ­¥å¢žåŠ éš¾åº¦
4. æ ¼å¼æ£€éªŒï¼šä»»åŠ¡éœ€è¦æ£€éªŒAgentæ˜¯å¦ä¸¥æ ¼éµå®ˆè¾“å‡ºæ ¼å¼è¦æ±‚

æ¯ä¸ªä»»åŠ¡åº”åŒ…å«ï¼š
- æ˜Žç¡®çš„åœºæ™¯èƒŒæ™¯
- å…·ä½“çš„è¾“å…¥è¦æ±‚ï¼ˆæ•°æ®ã€å‚æ•°ç­‰ï¼‰
- è¾“å‡ºæ ¼å¼å’Œå†…å®¹è¦æ±‚
- çº¦æŸæ¡ä»¶ï¼ˆå­—æ•°ã€æ ¼å¼ã€å¿…é¡»åŒ…å«çš„å…ƒç´ ç­‰ï¼‰"""
        
        user = f"""è§’è‰²ï¼š{self.agent['role']}
ç”¨æˆ·éœ€æ±‚ï¼š{self.agent['user_requirement']}
è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š{self.agent['output_format']}

è¯·ç”Ÿæˆ{num_tasks}ä¸ªè¯¦ç»†çš„æµ‹è¯•ä»»åŠ¡ã€‚æ¯ä¸ªä»»åŠ¡ç”¨"ã€ä»»åŠ¡Nã€‘"å¼€å¤´ï¼Œä»»åŠ¡ä¹‹é—´ç”¨ç©ºè¡Œåˆ†éš”ã€‚

ç¤ºä¾‹æ ¼å¼ï¼š
ã€ä»»åŠ¡1ã€‘
åœºæ™¯ï¼šæŸç”µå•†å¹³å°éœ€è¦ä¸ºæ–°å“æ‰‹æœºæ’°å†™äº§å“æè¿°
è¾“å…¥ä¿¡æ¯ï¼šå“ç‰Œä¸º"æ˜Ÿè€€"ï¼Œåž‹å·X1ï¼Œä¸»è¦å–ç‚¹åŒ…æ‹¬6.7è‹±å¯¸AMOLEDå±å¹•ã€5000mAhç”µæ± ã€1äº¿åƒç´ ä¸»æ‘„
è¾“å‡ºè¦æ±‚ï¼šæ’°å†™150-200å­—çš„äº§å“æè¿°ï¼Œä½¿ç”¨Markdownæ ¼å¼ï¼ŒåŒ…å«æ ‡é¢˜ã€æ ¸å¿ƒå–ç‚¹åˆ—è¡¨ã€è´­ä¹°å¼•å¯¼è¯­
çº¦æŸæ¡ä»¶ï¼šå¿…é¡»åŒ…å«"æ——èˆ°ä½“éªŒ"å’Œ"è¶…é•¿ç»­èˆª"ä¸¤ä¸ªå…³é”®è¯

è¯·æŒ‰ç…§ä¸Šè¿°æ ¼å¼ç”Ÿæˆ{num_tasks}ä¸ªä»»åŠ¡ï¼Œéš¾åº¦ä»Žç®€å•åˆ°å¤æ‚é€’è¿›ã€‚"""
        
        response = self.task_gen_client.chat([
            {"role": "system", "content": system},
            {"role": "user", "content": user}
        ], temperature=0.8)
        
        import re
        task_pattern = r'ã€ä»»åŠ¡\d+ã€‘(.*?)(?=ã€ä»»åŠ¡\d+ã€‘|$)'
        matches = re.findall(task_pattern, response, re.DOTALL)
        
        if matches:
            tasks = [m.strip() for m in matches if m.strip()]
        else:
            tasks = [line.strip() for line in response.split('\n') if line.strip() and len(line.strip()) > 20]
        
        tasks = tasks[:num_tasks]
        
        for t in tasks:
            self.db.add_task(self.agent_id, t)
        
        self.log(f"ç”Ÿæˆäº† {len(tasks)} ä¸ªè¯¦ç»†æµ‹è¯•ä»»åŠ¡")
        return tasks
    
    def run_task(self, agent_executor, task: str) -> Dict[str, Any]:
        self.current_task = task
        self.log(f"æ‰§è¡Œä»»åŠ¡: {task[:50]}...")
        
        try:
            output = agent_executor.invoke({"input": task})
            output_text = output.get('output', str(output))
        except Exception as e:
            output_text = f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥ï¼š{str(e)}"
            self.log(f"ä»»åŠ¡æ‰§è¡Œé”™è¯¯: {str(e)}")
        
        format_check = self.format_checker.check(output_text, self.agent['output_format'])
        evaluation = self.evaluator.evaluate(task, output_text)
        
        self.log(f"ä»»åŠ¡è¯„åˆ†: {evaluation.final_score}")
        
        return {
            "task": task,
            "output": output_text,
            "evaluation": evaluation,
            "format_check": format_check
        }
    
    def improve_prompt(self, old_prompt: str, results: List[Dict], avg_score: float) -> str:
        self.status = "ä¼˜åŒ–Prompt"
        self.log("æ­£åœ¨ä¼˜åŒ–Prompt...")
        
        system = "ä½ æ˜¯ä¸€ä¸ªPromptä¼˜åŒ–ä¸“å®¶ã€‚æ ¹æ®æµ‹è¯•åé¦ˆä¿®æ”¹promptï¼Œåªè¾“å‡ºæ–°promptã€‚"
        
        feedback_summary = "\n".join([
            f"ä»»åŠ¡ï¼š{r['task']}\n"
            f"è¯„åˆ†ï¼š{r['evaluation'].final_score}\n"
            f"æ ¼å¼æ£€æŸ¥ï¼š{r['format_check']}\n"
            f"åé¦ˆï¼š{r['evaluation'].feedback}"
            for r in results
        ])
        
        user = f"""å½“å‰promptï¼š
{old_prompt}

å¹³å‡åˆ†ï¼š{avg_score}
åé¦ˆè¯¦æƒ…ï¼š
{feedback_summary}

è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š{self.agent['output_format']}

è¯·ç”Ÿæˆæ”¹è¿›åŽçš„æ–°promptï¼Œç‰¹åˆ«å…³æ³¨ï¼š
1. æé«˜å†…å®¹è´¨é‡
2. ç¡®ä¿æ ¼å¼éµå¾ª
3. ä¼˜åŒ–å·¥å…·ä½¿ç”¨æŒ‡å¯¼

åªè¾“å‡ºæ–°promptï¼Œä¸è¦è§£é‡Šã€‚"""
        
        response = self.optimizer_client.chat([
            {"role": "system", "content": system},
            {"role": "user", "content": user}
        ])
        self.log("Promptä¼˜åŒ–å®Œæˆ")
        return response
    
    def run(self, max_iterations: int = 5, score_threshold: float = 4.5):
        try:
            self.db.update_agent_status(self.agent_id, 'running')
            
            tasks = self.generate_tasks()
            current_prompt = self.generate_initial_prompt()
            self.prompt = current_prompt
            
            score_history = []
            
            for iteration in range(1, max_iterations + 1):
                if self.stop_requested:
                    self.status = "å·²åœæ­¢"
                    self.log("ç”¨æˆ·è¯·æ±‚åœæ­¢")
                    self.db.update_agent_status(self.agent_id, 'paused', current_prompt)
                    break
                
                self.current_iteration = iteration
                self.status = f"è¿­ä»£ {iteration}/{max_iterations}"
                self.log(f"å¼€å§‹ç¬¬ {iteration} æ¬¡è¿­ä»£")
                
                agent_executor = create_agent(
                    system_prompt=current_prompt,
                    tools=self.tools,
                    model_client=self.base_client
                )
                
                results = []
                self.current_results = []
                for task in tasks:
                    if self.stop_requested:
                        break
                    result = self.run_task(agent_executor, task)
                    results.append(result)
                    self.current_results.append(result)
                
                if not results:
                    break
                
                avg_score = sum(r['evaluation'].final_score for r in results) / len(results)
                self.avg_score = avg_score
                score_history.append(avg_score)
                self.log(f"è¿­ä»£ {iteration} å¹³å‡åˆ†: {avg_score:.2f}")
                
                self.iteration_results.append({
                    "iteration": iteration,
                    "prompt": current_prompt,
                    "results": results.copy(),
                    "avg_score": avg_score
                })
                
                self._save_iteration(iteration, current_prompt, results, avg_score)
                
                if avg_score >= score_threshold:
                    self.status = "å®Œæˆ - è¾¾åˆ°è¯„åˆ†é˜ˆå€¼"
                    self.log(f"è¾¾åˆ°è¯„åˆ†é˜ˆå€¼ {score_threshold}")
                    self.db.update_agent_status(self.agent_id, 'completed', current_prompt)
                    self.completed = True
                    break
                
                if avg_score == 5.0:
                    self.status = "å®Œæˆ - æ»¡åˆ†"
                    self.log("èŽ·å¾—æ»¡åˆ†")
                    self.db.update_agent_status(self.agent_id, 'completed', current_prompt)
                    self.completed = True
                    break
                
                patience = self.agent.get('early_stop_patience', 3)
                threshold = self.agent.get('early_stop_threshold', 0.1)
                
                if len(score_history) > patience:
                    recent = score_history[-patience:]
                    improvements = [recent[i] - recent[i-1] for i in range(1, len(recent))]
                    if all(imp < threshold for imp in improvements):
                        self.status = "å®Œæˆ - æ—©åœ"
                        self.log("è§¦å‘æ—©åœæ¡ä»¶")
                        self.db.update_agent_status(self.agent_id, 'completed', current_prompt)
                        self.completed = True
                        break
                
                current_prompt = self.improve_prompt(current_prompt, results, avg_score)
                self.prompt = current_prompt
            
            if not self.completed and not self.stop_requested:
                self.status = "å®Œæˆ - è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°"
                self.db.update_agent_status(self.agent_id, 'completed', current_prompt)
                self.completed = True
            
            self.log("ä¼˜åŒ–å®Œæˆ")
            
        except Exception as e:
            self.error = str(e)
            self.status = f"é”™è¯¯: {str(e)}"
            self.log(f"é”™è¯¯: {str(e)}")
            self.db.update_agent_status(self.agent_id, 'failed')
    
    def _save_iteration(self, iteration: int, prompt: str, results: List[Dict], avg_score: float):
        conn = sqlite3.connect(settings.DB_PATH)
        cursor = conn.cursor()
        
        cursor.execute(
            """INSERT INTO iterations 
               (agent_id, iteration_number, prompt, avg_score, scores_detail)
               VALUES (?, ?, ?, ?, ?)""",
            (self.agent_id, iteration, prompt, avg_score, json.dumps({
                "avg": avg_score
            }))
        )
        iter_id = cursor.lastrowid
        
        for r in results:
            cursor.execute(
                """INSERT INTO task_results
                   (iteration_id, task_description, output, scores, final_score, 
                    feedback, format_check)
                   VALUES (?, ?, ?, ?, ?, ?, ?)""",
                (iter_id, r['task'], r['output'][:5000], 
                 json.dumps({"score": r['evaluation'].final_score}),
                 r['evaluation'].final_score, r['evaluation'].feedback,
                 r['format_check'])
            )
        
        conn.commit()
        conn.close()


def main():
    init_db(settings.DB_PATH)
    settings.setup_file_dir()
    
    st.sidebar.title("ðŸ¤– Agent Promptä¼˜åŒ–ç³»ç»Ÿ")
    page = st.sidebar.radio(
        "å¯¼èˆª",
        ["åˆ›å»ºæ–°Agent", "åŽ†å²è®°å½•", "è®¾ç½®"]
    )
    
    if page == "åˆ›å»ºæ–°Agent":
        show_create_page()
    elif page == "åŽ†å²è®°å½•":
        show_history_page()
    elif page == "è®¾ç½®":
        show_settings_page()


def show_create_page():
    st.header("åˆ›å»ºæ–°Agent")
    
    with st.form("agent_form"):
        col1, col2 = st.columns(2)
        
        with col1:
            role = st.text_input("Agentè§’è‰²", placeholder="å¦‚ï¼šæ–‡æ¡ˆç­–åˆ’")
            user_requirement = st.text_area(
                "éœ€æ±‚æè¿°",
                height=150,
                placeholder="è¯¦ç»†æè¿°Agentéœ€è¦å®Œæˆçš„ä»»åŠ¡å’Œèƒ½åŠ›è¦æ±‚"
            )
        
        with col2:
            output_formats = st.multiselect(
                "è¾“å‡ºæ ¼å¼",
                ["Markdown", "JSON", "YAML", "XML", "CSV", "çº¯æ–‡æœ¬"],
                default=["Markdown"]
            )
            custom_format = st.text_input(
                "è‡ªå®šä¹‰æ ¼å¼è¦æ±‚",
                placeholder="å¦‚æœ‰ç‰¹æ®Šæ ¼å¼è¦æ±‚ï¼Œè¯·åœ¨æ­¤æè¿°"
            )
        
        st.subheader("å‚æ•°é…ç½®")
        col3, col4, col5 = st.columns(3)
        
        with col3:
            max_iterations = st.number_input(
                "æœ€å¤§è¿­ä»£æ¬¡æ•°",
                min_value=1,
                max_value=20,
                value=settings.MAX_ITERATIONS
            )
            score_threshold = st.slider(
                "è¯„åˆ†é˜ˆå€¼",
                min_value=3.0,
                max_value=5.0,
                value=settings.SCORE_THRESHOLD,
                step=0.1
            )
        
        with col4:
            early_stop_patience = st.number_input(
                "æ—©åœè€å¿ƒå€¼",
                min_value=1,
                max_value=10,
                value=settings.EARLY_STOP_PATIENCE,
                help="è¿žç»­å¤šå°‘æ¬¡æ— æå‡æ—¶åœæ­¢"
            )
            early_stop_threshold = st.slider(
                "æ—©åœé˜ˆå€¼",
                min_value=0.0,
                max_value=1.0,
                value=settings.EARLY_STOP_THRESHOLD,
                step=0.05,
                help="æå‡å¹…åº¦ä½ŽäºŽæ­¤å€¼è§†ä¸ºæ— æå‡"
            )
        
        with col5:
            if 'ollama_models' not in st.session_state:
                st.session_state['ollama_models'] = get_ollama_models(settings.OLLAMA_BASE_URL.replace('/v1', ''))
            
            ollama_models = st.session_state['ollama_models']
            
            if not ollama_models:
                st.warning("æ— æ³•è¿žæŽ¥Ollamaï¼Œè¯·ç¡®ä¿Ollamaæ­£åœ¨è¿è¡Œ")
                base_model = st.text_input("åŸºç¡€æ¨¡åž‹", value=settings.DEFAULT_MODEL, key="base_model_text")
                eval_model = st.text_input("è¯„ä¼°æ¨¡åž‹", value=settings.DEFAULT_MODEL, key="eval_model_text")
            else:
                if 'base_model_index' not in st.session_state:
                    if settings.DEFAULT_MODEL in ollama_models:
                        st.session_state['base_model_index'] = ollama_models.index(settings.DEFAULT_MODEL)
                    else:
                        st.session_state['base_model_index'] = 0
                
                if 'eval_model_index' not in st.session_state:
                    if settings.DEFAULT_MODEL in ollama_models:
                        st.session_state['eval_model_index'] = ollama_models.index(settings.DEFAULT_MODEL)
                    else:
                        st.session_state['eval_model_index'] = 0
                
                base_model = st.selectbox(
                    "åŸºç¡€æ¨¡åž‹",
                    options=ollama_models,
                    index=st.session_state['base_model_index'],
                    key="base_model_select"
                )
                
                eval_model = st.selectbox(
                    "è¯„ä¼°æ¨¡åž‹",
                    options=ollama_models,
                    index=st.session_state['eval_model_index'],
                    key="eval_model_select"
                )
                
                st.session_state['base_model_index'] = ollama_models.index(base_model) if base_model in ollama_models else 0
                st.session_state['eval_model_index'] = ollama_models.index(eval_model) if eval_model in ollama_models else 0
        
        submitted = st.form_submit_button("å¼€å§‹ç”Ÿæˆ", type="primary")
    
    col_refresh, col_spacer = st.columns([1, 4])
    with col_refresh:
        if st.button("ðŸ”„ åˆ·æ–°æ¨¡åž‹åˆ—è¡¨"):
            st.session_state['ollama_models'] = get_ollama_models(settings.OLLAMA_BASE_URL.replace('/v1', ''))
            st.rerun()
    
    if submitted:
        if not role or not user_requirement:
            st.error("è¯·å¡«å†™è§’è‰²å’Œéœ€æ±‚æè¿°")
            return
        
        output_format = ", ".join(output_formats)
        if custom_format:
            output_format += f" ({custom_format})"
        
        db = Database()
        agent_id = db.create_agent(
            role=role,
            user_requirement=user_requirement,
            output_format=output_format,
            early_stop_patience=early_stop_patience,
            early_stop_threshold=early_stop_threshold
        )
        
        for model_type, model_name in [
            ('base', base_model),
            ('evaluator', eval_model),
            ('optimizer', base_model),
            ('task_generator', base_model)
        ]:
            db.add_model_config(
                agent_id=agent_id,
                model_type=model_type,
                model_source='ollama',
                model_name=model_name
            )
        
        st.session_state['current_agent_id'] = agent_id
        st.session_state['optimization_running'] = True
        st.rerun()
    
    agent_id = st.session_state.get('current_agent_id')
    if agent_id:
        show_running_page(agent_id)


def show_running_page(agent_id: int):
    st.header("ðŸ”„ ä¼˜åŒ–è¿›è¡Œä¸­")
    
    states = get_or_create_optimization_state()
    
    if agent_id not in states:
        db = Database()
        runner = OptimizationRunner(agent_id, db)
        states[agent_id] = runner
        
        max_iter = settings.MAX_ITERATIONS
        threshold = settings.SCORE_THRESHOLD
        
        def run_optimization():
            runner.run(max_iterations=max_iter, score_threshold=threshold)
        
        thread = threading.Thread(target=run_optimization, daemon=True)
        thread.start()
        st.session_state['optimization_thread'] = thread
    
    runner = states.get(agent_id)
    
    if not runner:
        st.error("ä¼˜åŒ–çŠ¶æ€ä¸¢å¤±")
        return
    
    col1, col2, col3 = st.columns([2, 2, 1])
    with col1:
        st.metric("çŠ¶æ€", runner.status)
    with col2:
        st.metric("å½“å‰è¿­ä»£", f"{runner.current_iteration}")
    with col3:
        st.metric("å¹³å‡åˆ†", f"{runner.avg_score:.2f}")
    
    if st.button("â¹ åœæ­¢ä¼˜åŒ–", type="secondary"):
        runner.stop_requested = True
        st.info("å·²è¯·æ±‚åœæ­¢...")
    
    st.subheader("ðŸ“ å½“å‰Prompt")
    if runner.prompt:
        with st.expander("æŸ¥çœ‹Prompt", expanded=False):
            st.code(runner.prompt, language="markdown")
    
    st.subheader("ï¿½ å½“å‰è¿­ä»£ä»»åŠ¡ç»“æžœ")
    if runner.current_results:
        for idx, result in enumerate(runner.current_results, 1):
            with st.expander(f"ä»»åŠ¡ {idx}: {result['task'][:60]}...", expanded=False):
                st.markdown("**ä»»åŠ¡æè¿°:**")
                st.info(result['task'])
                
                st.markdown("**Agentè¾“å‡º:**")
                st.code(result['output'], language="markdown")
                
                col_a, col_b = st.columns(2)
                with col_a:
                    st.metric("è¯„åˆ†", f"{result['evaluation'].final_score:.2f}")
                with col_b:
                    format_status = "âœ… é€šè¿‡" if result['format_check'] else "âŒ æœªé€šè¿‡"
                    st.metric("æ ¼å¼æ£€æŸ¥", format_status)
                
                st.markdown("---")
                st.markdown("### ðŸ“Š è¯„åˆ†è¯¦æƒ…")
                
                eval_obj = result['evaluation']
                st.code(eval_obj.get_scoring_rules() if hasattr(eval_obj, 'get_scoring_rules') else """è¯„åˆ†è§„åˆ™:
- å†…å®¹è´¨é‡(40%): è¯„ä¼°å†…å®¹ä¸Žä»»åŠ¡çš„ç›¸å…³æ€§ã€å‡†ç¡®æ€§ã€å®Œæ•´æ€§
- æ ¼å¼ç¬¦åˆåº¦(25%): è¯„ä¼°æ˜¯å¦ä¸¥æ ¼éµå¾ªçº¦å®šçš„è¾“å‡ºæ ¼å¼
- å·¥å…·ä½¿ç”¨(20%): è¯„ä¼°å·¥å…·é€‰æ‹©å’Œè°ƒç”¨æ•ˆæžœ
- åˆ›æ„æ€§(15%): è¯„ä¼°è¾“å‡ºçš„åˆ›æ–°æ€§""", language="markdown")
                
                col1, col2, col3, col4, col5 = st.columns(5)
                with col1:
                    st.metric("å†…å®¹è´¨é‡", f"{eval_obj.content_quality}/5", help=eval_obj.content_quality_reason)
                with col2:
                    st.metric("æ·±åº¦å®Œæ•´æ€§", f"{eval_obj.depth_completeness}/5", help=eval_obj.depth_completeness_reason)
                with col3:
                    st.metric("æ ¼å¼ç¬¦åˆåº¦", f"{eval_obj.format_compliance}/5", help=eval_obj.format_compliance_reason)
                with col4:
                    st.metric("å·¥å…·ä½¿ç”¨", f"{eval_obj.tool_usage}/5", help=eval_obj.tool_usage_reason)
                with col5:
                    st.metric("åˆ›æ„æ€§", f"{eval_obj.creativity}/5", help=eval_obj.creativity_reason)
                
                if eval_obj.content_quality_reason:
                    with st.expander("ðŸ“ å„ç»´åº¦è¯„åˆ†ç†ç”±"):
                        st.markdown(f"**å†…å®¹è´¨é‡ ({eval_obj.content_quality}/5):** {eval_obj.content_quality_reason}")
                        st.markdown(f"**æ·±åº¦å®Œæ•´æ€§ ({eval_obj.depth_completeness}/5):** {eval_obj.depth_completeness_reason}")
                        st.markdown(f"**æ ¼å¼ç¬¦åˆåº¦ ({eval_obj.format_compliance}/5):** {eval_obj.format_compliance_reason}")
                        st.markdown(f"**å·¥å…·ä½¿ç”¨ ({eval_obj.tool_usage}/5):** {eval_obj.tool_usage_reason}")
                        st.markdown(f"**åˆ›æ„æ€§ ({eval_obj.creativity}/5):** {eval_obj.creativity_reason}")
                
                st.markdown("**æ€»ä½“åé¦ˆ:**")
                st.success(eval_obj.feedback)
    
    st.subheader("ï¿½ æ‰§è¡Œæ—¥å¿—")
    log_container = st.container()
    with log_container:
        for log in runner.logs[-20:]:
            st.text(log)
    
    st.subheader("ðŸ“ˆ è¯„åˆ†è¶‹åŠ¿")
    db = Database()
    iterations = db.get_iterations(agent_id)
    
    if iterations:
        fig = go.Figure()
        fig.add_trace(go.Scatter(
            x=[i['iteration_number'] for i in iterations],
            y=[i['avg_score'] for i in iterations],
            mode='lines+markers',
            name='å¹³å‡åˆ†',
            line=dict(color='#FF4B4B', width=2),
            marker=dict(size=10)
        ))
        fig.update_layout(
            xaxis_title="è¿­ä»£æ¬¡æ•°",
            yaxis_title="å¹³å‡åˆ†",
            yaxis_range=[0, 5.5],
            height=300
        )
        st.plotly_chart(fig, use_container_width=True)
    
    st.subheader("ðŸ“š åŽ†å²è¿­ä»£è¯¦æƒ…")
    if runner.iteration_results:
        for iter_data in runner.iteration_results:
            with st.expander(f"è¿­ä»£ {iter_data['iteration']} - å¹³å‡åˆ†: {iter_data['avg_score']:.2f}", expanded=False):
                st.markdown("**è¯¥è¿­ä»£Prompt:**")
                st.code(iter_data['prompt'], language="markdown")
                
                st.markdown("**ä»»åŠ¡æ‰§è¡Œç»“æžœ:**")
                for idx, result in enumerate(iter_data['results'], 1):
                    st.markdown(f"**ä»»åŠ¡ {idx}:**")
                    st.info(result['task'])
                    st.code(result['output'], language="markdown")
                    st.caption(f"è¯„åˆ†: {result['evaluation'].final_score:.2f} | æ ¼å¼: {'âœ…' if result['format_check'] else 'âŒ'}")
    
    if runner.completed or runner.error:
        if runner.error:
            st.error(f"ä¼˜åŒ–å¤±è´¥: {runner.error}")
        else:
            st.success(f"ä¼˜åŒ–å®Œæˆï¼æœ€ç»ˆçŠ¶æ€: {runner.status}")
        
        if st.button("æŸ¥çœ‹è¯¦æƒ…", type="primary"):
            st.session_state['optimization_running'] = False
            st.session_state['current_agent_id'] = agent_id
            st.rerun()
    
    if not runner.completed and not runner.error:
        time.sleep(2)
        st.rerun()


def show_history_page():
    st.header("åŽ†å²è®°å½•")
    
    db = Database()
    agents = db.list_agents()
    
    if not agents:
        st.info("æš‚æ— åŽ†å²è®°å½•")
        return
    
    for agent in agents:
        with st.expander(f"#{agent['id']} {agent['role']} - {agent['status']} ({agent['created_at']})"):
            st.markdown(f"**è¾“å‡ºæ ¼å¼**: {agent['output_format']}")
            st.markdown(f"**éœ€æ±‚**: {agent['user_requirement']}")
            
            iterations = db.get_iterations(agent['id'])
            
            if iterations:
                df = pd.DataFrame(iterations)
                st.dataframe(df[['iteration_number', 'avg_score', 'created_at']])
            
            if agent['final_prompt']:
                st.markdown("**æœ€ç»ˆPrompt**:")
                st.code(agent['final_prompt'], language="markdown")


def show_settings_page():
    st.header("è®¾ç½®")
    
    st.subheader("Ollamaé…ç½®")
    ollama_url = st.text_input("Ollamaåœ°å€", value=settings.OLLAMA_BASE_URL)
    
    st.subheader("æœç´¢å¼•æ“Žé…ç½®")
    search_api_key = st.text_input("Google API Key", type="password", value=settings.GOOGLE_API_KEY or "")
    search_cse_id = st.text_input("Google CSE ID", value=settings.GOOGLE_CSE_ID or "")
    
    st.subheader("å®‰å…¨é…ç½®")
    file_dir = st.text_input("å…è®¸çš„æ–‡ä»¶ç›®å½•", value=settings.AGENT_FILE_DIR)
    
    st.subheader("é»˜è®¤å‚æ•°")
    col1, col2 = st.columns(2)
    with col1:
        default_max_iter = st.number_input("é»˜è®¤æœ€å¤§è¿­ä»£æ¬¡æ•°", value=settings.MAX_ITERATIONS)
        default_threshold = st.slider("é»˜è®¤è¯„åˆ†é˜ˆå€¼", min_value=3.0, max_value=5.0, value=settings.SCORE_THRESHOLD)
    
    with col2:
        default_patience = st.number_input("é»˜è®¤æ—©åœè€å¿ƒå€¼", value=settings.EARLY_STOP_PATIENCE)
        default_early_threshold = st.slider("é»˜è®¤æ—©åœé˜ˆå€¼", min_value=0.0, max_value=1.0, value=settings.EARLY_STOP_THRESHOLD)
    
    if st.button("ä¿å­˜è®¾ç½®"):
        st.success("è®¾ç½®å·²ä¿å­˜ï¼ˆæ³¨æ„ï¼šå½“å‰ä¼šè¯æœ‰æ•ˆï¼Œæ°¸ä¹…ä¿å­˜è¯·ä¿®æ”¹.envæ–‡ä»¶ï¼‰")


if __name__ == "__main__":
    main()
